AWS Bigdata --- Day4
Trainer: Prashant Nair
============================================================================================

Agenda
------
Working with Serverless architecture
AWS Lambda
Solving previous assignments 
	1. Flume to S3 using KInesis Delivery Stream
	2. S3 to DynamoDB using Code



			Architecture in Cloud
			          |
		-----------------------------------------
		|					|
	Server-Based				Server-less
	Architecture				Architecture


You are involving a server(EC2 instance)	You can run the code on the fly
to execute an app/code for 			without instantiating a server(EC2 instance).
preprocessing or transformation

You are responsible for the 			AWS will be responsible for server uptime,
server uptime interms of			scalability, monitoring, dynamic decision
flexibility and scalability			making etc.





Server-Based Architecture
--------------------------

LogGenerator -----> Agent ----> Kinesis DataSream ----> EC2 Instance -----> DynamoDB
							(Consumer.py)


Server-Less Architecture
---------------------------

LogGenerator -----> Agent ----> Kinesis DataSream ----> AWS Lambda -----> DynamoDB
							(Consumer.py)


Create the setup -----> 20 mins !


AWS Lambda Lab (Perform this Exercise --- 20 mins)
===============

1. Spin an EC2 instance. Ensure its attached with Administrator access role. Setup LogGenerator and create essential folders.

Steps to Create a Data Stream

2. Go to Kinesis Dashboard > Click on Kinesis Data Stream > Create Data Stream
3. Set the Stream name  and Click on Create Data Stream

4. Append the below configuration in the agent file


{
      "filePattern": "/var/log/cadabra/*.log",
      "kinesisStream": "CadabraOrders",
      "partitionKeyOption": "RANDOM",
      "dataProcessingOptions": [
         {
            "optionName": "CSVTOJSON",
            "customFieldNames": ["InvoiceNo", "StockCode", "Description", "Quantity", "InvoiceDate", "UnitPrice", "Customer", "Country"]
         }
      ]
    }

Restarted the agent

	sudo service aws-kinesis-agent restart

Setup Amazon DynamoDB

	Table: CadabraOrders
	Primary Key: CustomerID ,Datatype: Number
	Sort Key: OrderID , Datatype: String

5. Setup IAM for Lambda Function
	- Create a new Role for Lambda named CadabraOrdersLambdaPermission
	- Kinesis --- Full  (AmazonKinesisFullAccess)
	- Amazon DynamoDB --- Read/Write (AmazonDynamoDBFullAccess)
	- AWSLambdaDefaultExecutionRole


6. Setup Lambda Function
	- Go to Lambda Dashboard
	- Click on Create Function
	- Click Author From Scratch > Function Name: CadabraOrderFetchKinesis2Dynamo > Runtime: Python 3.6
	- Change the default Execution Role > Use an existing role >CadabraOrdersLambdaPermission
	- Click on Create Function

7. To make the Lambda Function work properly, we need to setup a trigger. In this scenario, the trigger can be AWS Kinesis Data Stream > CadabraOrders
	(Goal: Whenever any new data arrives to CadabraOrders data stream, the function must execute 	and transfer the data to DynamoDB)

	To configure the trigger, click on Add Trigger
	Under Trigger Conf, select Kinesis
	Select CadabraOrders and keep remaining options default.
	Click on Add.

8. Its time to copy paste the code. Click on Layers in Lambda Function and paste the code in the editor

9. We will initiate a data transfer so that the trigger will be enabled

	- Go to EC2 prompt and 
		sudo ./LogGenerator.py 100000

10. Check whether the data received in Amazon DynamoDB






1. Flume to S3 using KInesis Delivery Stream

Source Data ----> Flume ----> Destination  


Use a new ec2 instance !!! 


Complete the entire assignment !!! (20 mins)






































