AWS Bigdata ------- Day 2
Trainer: Prashant Nair 
============================================================================================

Agenda
-------
Storing with realtime stream input using Kinesis and S3
Understanding Kinesis Firehose Configurations
Working with Kinesis Data Streams
Demo on Data Acq and Data Preprocessing in Kinesis Data Streams




Data Acquisition ----> Streaming Data


			 	AWS Kinesis
				     |
		-------------------------------------------------
		|						|
	Data Acquisition				Data Transformation
	        |						|
	--------------------------			AWS Kinesis Analytics
	|			 |
AWS Kinesis 		AWS Kinesis
Firehose		Data Stream



Kinesis Firehose: Is a delivery stream !!! It can only do collect and store !! It's not                   capable to perform data preprocessing.


Kinesis Data Stream: It allows to 
			1. get the realtime stream data, 
			2. perform preprocessing on the fly 
			3. store it in his in-app storage

	This service is very similar to Apache Kafka (Pub-Sub)


Ref: https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html

Steps to Create a Data Stream
=================================

1. Go to Kinesis Dashboard > Click on Kinesis Data Stream > Create Data Stream
2. Set the Stream name  and Click on Create Data Stream

3. Append the below configuration in the agent file


{
      "filePattern": "/var/log/cadabra/*.log",
      "kinesisStream": "CadabraOrders",
      "partitionKeyOption": "RANDOM",
      "dataProcessingOptions": [
         {
            "optionName": "CSVTOJSON",
            "customFieldNames": ["InvoiceNo", "StockCode", "Description", "Quantity", "InvoiceDate", "UnitPrice", "Customer", "Country"]
         }
      ]
    }

4. Restarted the agent

	sudo service aws-kinesis-agent restart


Confirm your presence by saying JOINED in chatbox !!!





End to End Data Acq and Preprocessing Phase
===========================================


InputStream ----> Agent -----> DataStream1 ------> DeliveryStream1 -----> S3

Mini Project 1 -- Flume to S3 using AWS Services
==================================================

Source Dir -->Flume ---> Dest Dir----> Kinesis Agent ---> TransactionDataStream ---> KinesisFirehose ---------> S3
(TransactionDelivery)



Hints:
1. Download Flume

wget https://dlcdn.apache.org/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz

2. Install Flume

tar -xvzf apache-flume-1.9.0-bin.tar.gz

3. Rename the extracted folder

mv apache-flume-1.9.0-bin flume

4. Install Java

sudo yum install -y java-1.8.0-openjdk

5. Check if Java is working

java -version

6. Create source and destination folder

mkdir /home/ec2-user/source
sudo mkdir /var/log/transactions
sudo chown -R $USER:$GROUP /var/log/transactions


export JAVA_HOME= /usr/lib/jvm/java-openjdk/



7. Create a configuration for getting the data from /home/ec2-user/source and send it to /var/log/transactions

flumeFolderTrf.conf


8. Run Flume

flume/bin/flume-ng agent -n AgentName -f flumeFolderTrf.conf


9. Install AWS Kinesis Agent
10. Configure Kinesis Agent to get data from /var/log/transactions and provide it to TransactionDataStream.


































